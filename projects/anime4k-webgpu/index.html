<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Anime4K for WebGPU | Yuanqi Wang </title> <meta name="author" content="Yuanqi Wang"> <meta name="description" content="Client-side video upscaling in your browser. Built with WebGPU, WGSL, and TypeScript."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BB%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.yqwong.com/projects/anime4k-webgpu/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuanqi</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Anime4K for WebGPU</h1> <p class="post-description">Client-side video upscaling in your browser. Built with WebGPU, WGSL, and TypeScript.</p> </header> <article> <p><a href="https://anime4k-webgpu-demo.fly.dev/" rel="external nofollow noopener" target="_blank">Demo</a> | <a href="https://github.com/Anime4KWebBoost/Anime4K-WebGPU" rel="external nofollow noopener" target="_blank">Source</a></p> <h2 id="highlights">Highlights</h2> <ul> <li> <strong>Real-time Upscaling</strong>: Full client-side, cross-platform, hardware accelerated video upscaling in your browser.</li> <li> <strong>WebGPU Compute Shaders</strong>: Utilizes WebGPU compute shaders for fast and efficient processing.</li> <li> <strong>Easy Integration</strong>: Simple API for easy integration into your WebGPU pipeline.</li> </ul> <div align="center"> <img src="/assets/img/projects/anime4k-webgpu/intro.gif" style="width: 60%; height: auto;"> <p>Real-time Upscaling. Original (left) vs. Upscaled (right)</p> </div> <h2 id="introduction">Introduction</h2> <p>An <a href="https://github.com/bloc97/Anime4K" rel="external nofollow noopener" target="_blank">Anime4K</a> implementation for WebGPU, featuring video enhancements including upscaling, denoising, and deblurring. Computing is done entirely on the client side using WebGPU compute shaders. Functionality of this implementation is published as an <a href="https://www.npmjs.com/package/anime4k-webgpu" rel="external nofollow noopener" target="_blank">NPM package</a>, and can be easily incorporated into your WebGPU pipeline.</p> <p>Take a look at our web demo at https://anime4k-webgpu-demo.fly.dev/ (<a href="https://github.com/Anime4KWebBoost/Anime4K-Web-Demo" rel="external nofollow noopener" target="_blank">Source</a>)</p> <p>Note: your browser must support WebGPU. See this <a href="https://caniuse.com/webgpu" rel="external nofollow noopener" target="_blank">list</a> for compatibility.</p> <hr> <h2 id="usage">Usage</h2> <p>There are 2 ways to use this package:</p> <h3 id="1-render-wrapper">1. Render Wrapper</h3> <p>This is for frontend devs who do not wish to tap into WebGPU too much. An React example can be found <a href="https://github.com/Anime4KWebBoost/Anime4K-WebGPU/blob/main/examples/react-renderer.tsx" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>You only need the <code class="language-plaintext highlighter-rouge">render</code> function which will setup all the rendering from a video element to a canvas element:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="p">{</span> <span class="nx">CNNx2UL</span><span class="p">,</span> <span class="nx">GANUUL</span><span class="p">,</span> <span class="nx">render</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">anime4k-webgpu</span><span class="dl">"</span><span class="p">;</span>

<span class="k">await</span> <span class="nf">render</span><span class="p">({</span>
  <span class="c1">// your source video HTMLElement</span>
  <span class="nx">video</span><span class="p">,</span>
  <span class="c1">// your render destination canvas HTMLElement</span>
  <span class="nx">canvas</span><span class="p">,</span>
  <span class="c1">// your function to build custom pipeline</span>
  <span class="c1">// return all pipelines in order of execution</span>
  <span class="c1">// e.g. inputTexture(video) -&gt; CNNx2UL -&gt; GANUUL -&gt; (canvas)</span>
  <span class="na">pipelineBuilder</span><span class="p">:</span> <span class="p">(</span><span class="nx">device</span><span class="p">,</span> <span class="nx">inputTexture</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">const</span> <span class="nx">upscale</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">CNNx2UL</span><span class="p">({</span>
      <span class="nx">device</span><span class="p">,</span>
      <span class="nx">inputTexture</span><span class="p">,</span>
    <span class="p">});</span>
    <span class="kd">const</span> <span class="nx">restore</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">GANUUL</span><span class="p">({</span>
      <span class="nx">device</span><span class="p">,</span>
      <span class="na">inputTexture</span><span class="p">:</span> <span class="nx">upscale</span><span class="p">.</span><span class="nf">getOutputTexture</span><span class="p">(),</span>
    <span class="p">});</span>
    <span class="k">return</span> <span class="p">[</span><span class="nx">upscale</span><span class="p">,</span> <span class="nx">restore</span><span class="p">];</span>
  <span class="p">},</span>
<span class="p">});</span>
</code></pre></div></div> <p>In the upper example, the input texture (vide) will go through a <code class="language-plaintext highlighter-rouge">CNNx2UL</code> for upscaling, and then a <code class="language-plaintext highlighter-rouge">GANUUL</code> for restore, before it is rendered to the canvas. You will build your custom pipeline in the <code class="language-plaintext highlighter-rouge">pipelineBuilder</code> function.</p> <p>Alternativey, to use a <a href="https://github.com/bloc97/Anime4K/blob/master/md/GLSL_Instructions_Advanced.md" rel="external nofollow noopener" target="_blank">preset mode</a>, native texture resolution and render target resolution are needed to setup the correct pipeline combinations:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="p">{</span> <span class="nx">ModeA</span><span class="p">,</span> <span class="nx">render</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">anime4k-webgpu</span><span class="dl">"</span><span class="p">;</span>

<span class="k">await</span> <span class="nf">render</span><span class="p">({</span>
  <span class="nx">video</span><span class="p">,</span>
  <span class="nx">canvas</span><span class="p">,</span>
  <span class="c1">// inputTexture(video) -&gt; Mode A -&gt; (canvas)</span>
  <span class="na">pipelineBuilder</span><span class="p">:</span> <span class="p">(</span><span class="nx">device</span><span class="p">,</span> <span class="nx">inputTexture</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">const</span> <span class="nx">preset</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ModeA</span><span class="p">({</span>
      <span class="nx">device</span><span class="p">,</span>
      <span class="nx">inputTexture</span><span class="p">,</span>
      <span class="na">nativeDimensions</span><span class="p">:</span> <span class="p">{</span>
        <span class="na">width</span><span class="p">:</span> <span class="nx">video</span><span class="p">.</span><span class="nx">videoWidth</span><span class="p">,</span>
        <span class="na">height</span><span class="p">:</span> <span class="nx">video</span><span class="p">.</span><span class="nx">videoHeight</span><span class="p">,</span>
      <span class="p">},</span>
      <span class="na">targetDimensions</span><span class="p">:</span> <span class="p">{</span>
        <span class="na">width</span><span class="p">:</span> <span class="nx">canvas</span><span class="p">.</span><span class="nx">width</span><span class="p">,</span>
        <span class="na">height</span><span class="p">:</span> <span class="nx">canvas</span><span class="p">.</span><span class="nx">height</span><span class="p">,</span>
      <span class="p">},</span>
    <span class="p">});</span>
    <span class="k">return</span> <span class="p">[</span><span class="nx">preset</span><span class="p">];</span>
  <span class="p">},</span>
<span class="p">});</span>
</code></pre></div></div> <h3 id="2-webgpu-pipelines">2. WebGPU Pipelines</h3> <p>If you already have a webGPU render pipeline setup and would like to use Anime4K on an existing texture,</p> <p>This package contains classes that implements interface <code class="language-plaintext highlighter-rouge">Anime4KPipeline</code>. To use these classes, first install <code class="language-plaintext highlighter-rouge">anime4k-webgpu</code> package, then insert proveded pipelines in 4 lines:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// +++ import CNNx2UL, one of the CNN upscale pipeline +++</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">Anime4KPipeline</span><span class="p">,</span> <span class="nx">CNNx2UL</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">anime4k-webgpu</span><span class="dl">'</span><span class="p">;</span>

<span class="c1">// your original texture to be processed</span>
<span class="kd">const</span> <span class="nx">inputTexture</span><span class="p">:</span> <span class="nx">GPUTexture</span><span class="p">;</span>

<span class="c1">// +++ instantiate pipeline +++</span>
<span class="kd">const</span> <span class="nx">pipeline</span><span class="p">:</span> <span class="nx">Anime4KPipeline</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">CNNx2UL</span><span class="p">({</span>
  <span class="nx">device</span><span class="p">,</span>
  <span class="nx">inputTexture</span>
<span class="p">});</span>

<span class="c1">// bind (upscaled) output texture wherever you want e.g. render pipeline</span>
<span class="kd">const</span> <span class="nx">renderBindGroup</span> <span class="o">=</span> <span class="nx">device</span><span class="p">.</span><span class="nf">createBindGroup</span><span class="p">({</span>
  <span class="p">...</span>
  <span class="na">entries</span><span class="p">:</span> <span class="p">[{</span>
    <span class="na">binding</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="c1">// +++ use pipeline.getOutputTexture() instead of inputTexture +++</span>
    <span class="na">resource</span><span class="p">:</span> <span class="nx">pipeline</span><span class="p">.</span><span class="nf">getOutputTexture</span><span class="p">().</span><span class="nf">createView</span><span class="p">(),</span>
  <span class="p">}]</span>
<span class="p">});</span>

<span class="kd">function</span> <span class="nf">frame</span><span class="p">()</span> <span class="p">{</span>
  <span class="kd">const</span> <span class="nx">commandEncoder</span><span class="p">:</span> <span class="nx">GPUCommandEncoder</span><span class="p">;</span>

  <span class="c1">// +++ inject commands into the encoder +++</span>
  <span class="nx">pipeline</span><span class="p">.</span><span class="nf">pass</span><span class="p">(</span><span class="nx">commandEncoder</span><span class="p">);</span>

  <span class="c1">// begin other render pass...</span>
<span class="p">}</span>
</code></pre></div></div> <p>To change an adjustable parameter (e.g. deblur strength) call <code class="language-plaintext highlighter-rouge">Anime4KPipeline::updateParam(param: string, value: any)</code> and the value will be applied for the next render cycle:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pipeline</span><span class="p">.</span><span class="nf">updateParam</span><span class="p">(</span><span class="dl">"</span><span class="s2">strength</span><span class="dl">"</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">);</span>
</code></pre></div></div> <p>The input texture must have usage <code class="language-plaintext highlighter-rouge">TEXTURE_BINDING</code>, and the output texture has <code class="language-plaintext highlighter-rouge">TEXTURE_BINDING | RENDER_ATTACHMENT | STORAGE_BINDING</code> to be used in render pipelines. You can also have multiple pipelines in tandem to achieve sophisticated effects.</p> <h3 id="supported-pipelines">Supported Pipelines</h3> <p>This package currently support the following pipelines and presets (items marked as ❌ are still in progress):</p> <ul> <li>Deblur <ul> <li>✅ DoG</li> </ul> </li> <li>Denoise <ul> <li>✅ BilateralMean</li> </ul> </li> <li>Restore <ul> <li>✅ CNNM</li> <li>✅ CNNSoftM</li> <li>✅ CNNSoftVL</li> <li>✅ CNNVL</li> <li>✅ CNNUL</li> <li>✅ GANUUL</li> </ul> </li> <li>Upscale <ul> <li>✅ CNNx2M</li> <li>✅ CNNx2VL</li> <li>✅ DenoiseCNNx2VL</li> <li>✅ CNNx2UL</li> <li>✅ GANx3L</li> <li>✅ GANx4UUL</li> </ul> </li> <li>Other Helpers <ul> <li>✅ AutoDownscalePre</li> <li>✅ ClampHighlights</li> </ul> </li> <li>Preset Mode Collections (see also <a href="https://github.com/bloc97/Anime4K/blob/master/md/GLSL_Instructions_Advanced.md" rel="external nofollow noopener" target="_blank">link</a>) <ul> <li>✅ ModeA</li> <li>✅ ModeB</li> <li>✅ ModeC</li> <li>✅ ModeAA</li> <li>✅ ModeBB</li> <li>✅ ModeCA</li> </ul> </li> </ul> <hr> <h2 id="performance-analysis">Performance Analysis</h2> <h3 id="visualization-comparisons">Visualization Comparisons</h3> <p>Following comparisons are done with a 360p image as input. The image is from Anime4K repo.</p> <p><strong>1. Compare with Denoise &amp; Deblur effects</strong></p> <p>After applying denoise effect, the image become smoother. The intensity sigma is set to 0.2 and the spatial sigma is set to 2. Increasing intensity sigma will make bilateral filter approximat Gaussian convolution, and increasing the spatial sigma will make the color smoother.</p> <p>For the denoise effect, deblurring strength has been calibrated to a level of 7. As deblurring strength increasing, the effect of deblur is more distinct. This deblurring process enhances the video’s clarity by sharpening the image’s edges. However, it’s important to note that this enhancement may also inadvertently amplify aliasing effects.</p> <div align="center"> <img src="/assets/img/projects/anime4k-webgpu/1a.svg" style="width: 100%; height: auto;"> <p>Figure 1. Compare with Denoise &amp; Deblur effect</p> </div> <p><strong>2. Compare with restore CNN and restore GAN effects</strong></p> <p>Utilizing restoration models on the image enhances its clarity and sharpens its edges, yet it may also result in the introduction of some aliasing. The restore GAN model, being larger in size compared to the restore CNN model, excels in enhancing edge details. However, this comes with a trade-off as it tends to introduce a greater degree of aliasing. The restoration process will enhance the image’s clarity without altering its dimensions.</p> <div align="center"> <img src="/assets/img/projects/anime4k-webgpu/1b.svg" style="width: 100%; height: auto;"> <p>Figure 2. Compare with restore CNN and restore GAN</p> </div> <p><strong>3. Compare with upscale CNN (x2) and upscale GAN (x3 &amp; x4) and the Real-ESRGAN effects</strong></p> <p>Utilizing upscale CNN and upscale GAN techniques results in an increase in image size as these models aim to upscale the image, enhancing its resolution. Among these models, the upscale GAN x4 stands out as the most substantial, producing an output image that is four times larger in both width and height, yielding the most impressive results among the three upscale models. The Real-ESRGAN result is generated by <a href="https://github.com/sona1111/webgpu-super-resolution" rel="external nofollow noopener" target="_blank">webgpu-super-resolution</a>. We have upgraded the project for compatibility with the latest WebGPU standards, allowing the original image to be processed correctly. Although Real-ESRGAN delivers the sharpest image, it is significantly slower—approximately 1000 times more so. Our project is focused on achieving real-time video upscaling, which requires rapid processing. Fortunately, the upscale quality is satisfactory and well-suited for real-time video applications.</p> <div align="center"> <img src="/assets/img/projects/anime4k-webgpu/1c.svg" style="width: 100%; height: auto;"> <p>Figure 3. Compare with upscale CNN(x2) and upscale GAN (x3 &amp; x4) and Real-ESRGAN from </p> </div> <p><strong>4. Compare with upscale GAN (x4) + Restore GAN and the Real-ESRGAN effects</strong></p> <p>Input (1x):<br> The original image at 200x133 pixels, serving as the starting point for upscaling.</p> <p>Upscale GAN x4 + Restore GAN: Upscaling is achieved by a factor of 4 using a generative adversarial network (GAN), followed by a restoration process with another GAN. This method prioritizes speed and memory efficiency, operating 1000 times faster than Real-ESRGAN and with substantially lower memory requirements, at the cost of some visual quality.</p> <p>Real-ESRGAN x4:<br> The Real-ESRGAN result is generated by <a href="https://github.com/sona1111/webgpu-super-resolution" rel="external nofollow noopener" target="_blank">webgpu-super-resolution</a>. This image has been upscaled by a factor of 4 using the Real-ESRGAN technique. It offers enhanced visual quality that more closely approximates the ground truth but at a cost of slower runtime and higher memory usage, which may not be feasible for real-time applications.</p> <p>Ground Truth (x5):<br> The reference standard with a resolution five times that of the original input, against which the other images are compared.</p> <p>Conclusion:<br> The comparison highlights significant trade-offs between the speed of rendering, memory usage, and the visual quality of upscaled images. While the Upscale CNN method provides a rapid and memory-efficient solution suitable for real-time applications, the Real-ESRGAN approach delivers superior image quality, which may be necessary for applications where visual fidelity is paramount, albeit with greater resource requirements.</p> <div align="center"> <img src="/assets/img/projects/anime4k-webgpu/2.svg" style="width: 100%; height: auto;"> <p>Figure 4. Compare with Upscale CNN x2 + Retore CNN &amp; Real-ESRGAN effect</p> </div> <h3 id="run-time-comparisonsgpu">Run Time Comparisons(GPU)</h3> <p>We conducted comparisons using a video (Demo Video: Miss Kobayashi’s Dragon Maid) as our test input. The GPU processing time, measured in milliseconds per frame, was tracked using the GPU performance analysis tool in Chrome. This was achieved by recording a 10-second segment and then calculating the average GPU time per frame. This calculation was done by dividing the total GPU time over 10 seconds by the product of 10 seconds and the video’s frame rate.</p> <p><strong>1. GPU time for different effects in different graphic cards</strong></p> <p>The subsequent comparisons reveal that the GPU processing time for rendering a frame of texture is sufficiently swift for real-time video upscaling across all models with a 720p video input. While the RTX 4090 GPU exhibits a slightly quicker frame rendering time, the performance on the RTX 3070Ti is nearly on par, with both completing the task within 3 milliseconds. Looking ahead, as hardware technology continues to evolve, we anticipate that this project will greatly benefit from utilizing the cross-platform capabilities of WebGPU.</p> <div align="center"> <img src="/assets/img/projects/anime4k-webgpu/performance_analysis_720P_diff_graphic_card.svg" style="width: 100%; height: auto;"> <p>Figure 5. GPU time for different effects (720P)</p> </div> <p><strong>2. GPU time for different effects with different resolution video inputs</strong></p> <p>Following comparisons are done with RTX 3070Ti graphic card.</p> <p>With following plot, it becomes evident that each effect/rendering pipeline display a remarkable consistency in frame rendering time. This observation holds true irrespective of the input video size, indicating a well-optimized rendering process. Notably, the time required to render a single frame across these various effects is sufficiently brief, making it viable for real-time video application of all effects without significant delay.</p> <p>However, an exception is observed with the upscale GAN x4 when applied to a 1080p video input. In this case, the rendering time significantly increases. This outlier can be attributed to the upscale GAN x4’s ambitious task of upscaling the video to an 8K resolution. Such a substantial increase in resolution demands an intense computation, explaining the prolonged rendering time in this particular scenario. This suggests that while the system is generally efficient for real-time applications, certain high-intensity tasks like extreme upscaling to 8K can still pose challenges in maintaining the same level of performance.</p> <div align="center"> <img src="/assets/img/projects/anime4k-webgpu/performance_analysis_diff_input_size.svg" style="width: 100%; height: auto;"> <p>Figure 6. GPU time for different effects with different resolution videos</p> </div> <hr> <h2 id="future-improvements">Future Improvements</h2> <ul> <li> <p>Use <code class="language-plaintext highlighter-rouge">read-write</code> storage texture instead of <code class="language-plaintext highlighter-rouge">write-only</code> storage texture (Not yet supported in Chrome stable) for lower VRAM usage.</p> </li> <li> <p>Enhancing the Pipeline: Currently, our pipelines operate sequentially. As a future enhancement, we plan to restructure the system so that pipelines which are independent of each other can run concurrently, in parallel. This will optimize our process efficiency and performance.</p> </li> </ul> <hr> <h2 id="reference">Reference</h2> <p>This project references a variety of resources:</p> <ul> <li> <strong>Multimedia Demonstrations</strong> <ul> <li>Demo Video: Miss Kobayashi’s Dragon Maid: <a href="https://www.youtube.com/watch?v=NQF3a6A7kcQ" rel="external nofollow noopener" target="_blank">YouTube Video</a> </li> <li>Demo Image: Higurashi 360P from Anime4K <a href="https://github.com/bloc97/Anime4K/tree/master" rel="external nofollow noopener" target="_blank">(repo)</a> </li> </ul> </li> </ul> <hr> <h2 id="project-info">Project Info</h2> <p>University of Pennsylvania, CIS 565: GPU Programming and Architecture, Final Project</p> <p>Authors (alphabetical order with equal contribution):</p> <ul> <li>Ruijun(Daniel) Zhong <a href="https://www.linkedin.com/in/daniel-z-73158b152/" rel="external nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://www.danielzhongportfolio.com/" rel="external nofollow noopener" target="_blank">Personal Website</a> </li> <li>Tong Hu <a href="https://www.linkedin.com/in/tong-hu-5819a122a/" rel="external nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://www.tong-hu.com/" rel="external nofollow noopener" target="_blank">Personal Website</a> </li> <li>Yuanqi Wang <a href="https://www.linkedin.com/in/yuanqi-wang-414b26106/" rel="external nofollow noopener" target="_blank">LinkedIn</a> | <a href="https://github.com/plasmas" rel="external nofollow noopener" target="_blank">GitHub</a> | <a href="https://www.yqwong.com/">Personal Website</a> </li> </ul> <p>Following are related resources:</p> <ul> <li><a href="https://docs.google.com/presentation/d/1QKqbgSwnxgH9Htm_SM3CYOdL-5bIGNmq2VT5xSiAARs/edit?usp=sharing" rel="external nofollow noopener" target="_blank">Project Pitch Presentation</a></li> <li><a href="https://docs.google.com/presentation/d/1vmPm16IPomAWMxoYmIAXXQBsLOk9jNHT3xhsdn3K37Q/edit?usp=sharing" rel="external nofollow noopener" target="_blank">Milestone 1 Presentation</a></li> <li><a href="https://docs.google.com/presentation/d/1gor5TFMMb6vhkBVJBjttUPvZFZvytHT6YuLI_zqOfAc/edit?usp=sharing" rel="external nofollow noopener" target="_blank">Milestone 2 Presentation</a></li> <li><a href="https://docs.google.com/presentation/d/1nEbmPhS-CbPhUhzjQtkeCNdFZiAGyXShSNRu5my55iY/edit?usp=sharing" rel="external nofollow noopener" target="_blank">Milestone 3 Presentation</a></li> <li><a href="https://docs.google.com/presentation/d/1DGrMEzUCkYuYvQE-T6p7vRvCusKvNPK1/edit#slide=id.g1ed29a20d86_4_11" rel="external nofollow noopener" target="_blank">Final Presentation</a></li> </ul> <hr> <h2 id="credits">Credits</h2> <ul> <li><a href="https://github.com/bloc97/Anime4K" rel="external nofollow noopener" target="_blank">Anime4K</a></li> <li><a href="https://github.com/keijiro/UnityAnime4K" rel="external nofollow noopener" target="_blank">UnityAnime4K</a></li> <li><a href="https://github.com/webgpu/webgpu-samples" rel="external nofollow noopener" target="_blank">WebGPU-Samples</a></li> </ul> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Yuanqi Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 14, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>